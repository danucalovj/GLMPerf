{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predicting Airline Data using a Generalized Linear Model (GLM) in Spark\n",
    "\n",
    "In particular, we will predict the probability that a flight is late based on its departure date/time, the expected flight time and distance, the origin and destitation airports.\n",
    "\n",
    "The core library for the dataframe part is [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html).<br>\n",
    "The core library for the machine learning part is [Spark MLIB](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "\n",
    "Spark can be used from R and python, but has it is originally written in scala, we prefer to use it directly from the scala language.\n",
    "\n",
    "The [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala#spark) is used for this notebook\n",
    "\n",
    "### Considerations\n",
    "\n",
    "The objective of this notebook is to define a simple model offerring a point of comparison in terms of computing performances across datascience language and libraries.  In otherwords, this notebook is not for you if you are looking for the most accurate model in airline predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install and Load useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:03:49.413968",
     "start_time": "2017-02-28T13:03:47.230Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:01.543057",
     "start_time": "2017-02-28T13:03:54.260Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/loicus/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-nop/1.7.12/slf4j-nop-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/loicus/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.helpers.NOPLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    // adjust spark version - spark >= 1.6 should be fine, possibly >= 1.3 too\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    // JupyterSparkContext-s (SparkContext aware of the jupyter-scala kernel)\n",
       "//import $ivy.`co.theasi::plotly:0.2.0`\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.LogisticRegression\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark._\n",
       "\n",
       "// The conf can be tweaked a bit before use.\n",
       "// Mark it transient to prevent serialization issues.\n",
       "\u001b[39m\n",
       "\u001b[36msparkConf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@316fb8de\n",
       "\u001b[36msc\u001b[39m: \u001b[32mjupyter\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mJupyterSparkContext\u001b[39m = jupyter.spark.JupyterSparkContext@73da97a2\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@33d35811"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.slf4j:slf4j-nop:1.7.12`         // for cleaner logs\n",
    "import $ivy.`org.apache.spark::spark-sql:2.0.2`  // adjust spark version - spark >= 1.6 should be fine, possibly >= 1.3 too\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.0.2`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.0-RC5` // JupyterSparkContext-s (SparkContext aware of the jupyter-scala kernel)\n",
    "//import $ivy.`co.theasi::plotly:0.2.0`\n",
    "\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "import jupyter.spark._\n",
    "\n",
    "// The conf can be tweaked a bit before use.\n",
    "// Mark it transient to prevent serialization issues.\n",
    "@transient val sparkConf = new SparkConf()\n",
    "  .setAppName(\"SBTB\")\n",
    "  .setMaster(\"local[*]\") // use all local CPUs\n",
    "  .set(\"spark.executor.memory\", \"2g\")\n",
    "  .set(\"spark.driver.memory\", \"2g\")\n",
    "\n",
    "@transient val sc = new JupyterSparkContext(sparkConf)\n",
    "\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "- The dataset is taken from [http://stat-computing.org](http://stat-computing.org/dataexpo/2009/the-data.html).  We take the data corresponding to year 2008.\n",
    "- We restrict the dataset to the first million rows\n",
    "- We print all column names and the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:11.534056",
     "start_time": "2017-02-28T13:03:56.937Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdffull\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Year: int, Month: int ... 27 more fields]\n",
       "\u001b[36mcount\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m7009728L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dffull = sqlContext\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"inferSchema\", true)\n",
    "  .load(\"2008.csv\")\n",
    "val count = dffull.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:12.125937",
     "start_time": "2017-02-28T13:03:57.095Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Year: int, Month: int ... 27 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = dffull.sample(false, 1000000.toFloat/count) //spark way of randomly keeping (approximately) 1M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:12.384408",
     "start_time": "2017-02-28T13:03:57.233Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preparation for training\n",
    "\n",
    "- We turn origin/destination categorical data to a \"one-hot\" encoding representation\n",
    "- We create a new \"binary\" column indicating if the flight was delayed or not.\n",
    "- We show the first 5 rows of the modified dataset\n",
    "- We split the dataset in two parts:  a training dataset and a testing dataset containing 80% and 20% of the rows, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:13.249735",
     "start_time": "2017-02-28T13:03:57.527Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+-------+--------+------+----+--------+------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|AirTime|Distance|Origin|Dest|ArrDelay|IsArrDelayed|\n",
      "+----+-----+----------+---------+-------+-------+--------+------+----+--------+------------+\n",
      "|2008|    1|         3|        4|    617|     70|     451|   IND| MCI|       2|           1|\n",
      "|2008|    1|         3|        4|   1954|    155|    1093|   ISP| FLL|       4|           1|\n",
      "|2008|    1|         3|        4|    712|    142|     972|   ISP| MCO|      -7|           0|\n",
      "|2008|    1|         3|        4|   1312|    140|     972|   ISP| MCO|      -4|           0|\n",
      "|2008|    1|         3|        4|    634|    142|    1034|   ISP| TPA|     -28|           0|\n",
      "+----+-----+----------+---------+-------+-------+--------+------+----+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Year: int, Month: int ... 9 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df\n",
    "          .filter(row => row.getAs[Double](\"ArrDelay\")!=null)\n",
    "          .withColumn(\"IsArrDelayed\", (df(\"ArrDelay\")>0).cast(\"int\"))\n",
    "          .withColumn(\"DepTime\", df(\"DepTime\").cast(\"int\"))\n",
    "          .withColumn(\"AirTime\", df(\"AirTime\").cast(\"int\"))\n",
    "          .filter(row => row.getAs[Int](\"DepTime\")!=null && row.getAs[Int](\"AirTime\")!=null)\n",
    "          .select(\"Year\",\"Month\",  \"DayofMonth\" ,\"DayOfWeek\", \"DepTime\", \"AirTime\", \"Distance\", \"Origin\", \"Dest\", \"ArrDelay\", \"IsArrDelayed\")        \n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:26.286656",
     "start_time": "2017-02-28T13:03:57.675Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mOriginIndexer\u001b[39m: \u001b[32mStringIndexer\u001b[39m = strIdx_1ce73ff23548\n",
       "\u001b[36mOriginEncoder\u001b[39m: \u001b[32mOneHotEncoder\u001b[39m = oneHot_dc35da52f7f1\n",
       "\u001b[36mDestIndexer\u001b[39m: \u001b[32mStringIndexer\u001b[39m = strIdx_6aa37c5fb316\n",
       "\u001b[36mDestEncoder\u001b[39m: \u001b[32mOneHotEncoder\u001b[39m = oneHot_ed0606908b4e\n",
       "\u001b[36mAssembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_e82c463f074a\n",
       "\u001b[36mpipeline\u001b[39m: \u001b[32mPipeline\u001b[39m = pipeline_07787b63f053\n",
       "\u001b[36mPreparator\u001b[39m: \u001b[32mPipelineModel\u001b[39m = pipeline_07787b63f053\n",
       "\u001b[36mdfPrepared\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Year: int, Month: int ... 14 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val OriginIndexer = new StringIndexer()\n",
    "  .setInputCol(\"Origin\")\n",
    "  .setOutputCol(\"OriginIndex\")\n",
    "\n",
    "val OriginEncoder = new OneHotEncoder()\n",
    "  .setInputCol(\"OriginIndex\")\n",
    "  .setOutputCol(\"OriginVec\")\n",
    "\n",
    "val DestIndexer = new StringIndexer()\n",
    "  .setInputCol(\"Dest\")\n",
    "  .setOutputCol(\"DestIndex\")\n",
    "\n",
    "val DestEncoder = new OneHotEncoder()\n",
    "  .setInputCol(\"DestIndex\")\n",
    "  .setOutputCol(\"DestVec\")\n",
    "\n",
    "val Assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"Year\",\"Month\",  \"DayofMonth\" ,\"DayOfWeek\", \"DepTime\", \"AirTime\", \"Distance\", \"OriginVec\", \"DestVec\"))\n",
    "  .setOutputCol(\"Features\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(OriginIndexer, OriginEncoder, DestIndexer, DestEncoder, Assembler))\n",
    "\n",
    "val Preparator = pipeline.fit(df2)\n",
    "val dfPrepared = Preparator.transform(df2).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:32.688200",
     "start_time": "2017-02-28T13:03:58.903Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            Features|IsArrDelayed|\n",
      "+--------------------+------------+\n",
      "|(610,[0,1,2,3,4,5...|           1|\n",
      "|(610,[0,1,2,3,4,5...|           1|\n",
      "|(610,[0,1,2,3,4,5...|           1|\n",
      "|(610,[0,1,2,3,4,5...|           1|\n",
      "|(610,[0,1,2,3,4,5...|           0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Year: int, Month: int ... 14 more fields]\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Year: int, Month: int ... 14 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = dfPrepared.randomSplit(Array(0.8,0.2))\n",
    "train.select(\"Features\",\"IsArrDelayed\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model building\n",
    "\n",
    "- We define the generalized linear model using a binomial function --> Logistic regression.\n",
    "- We train the model and measure the training time --> ~15sec using the 8 cores of an intel i7-6700K (4.0 GHz) for 800K rows \t\n",
    "- We show the model coefficients\n",
    "- We show the 10 most important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:04:43.503268",
     "start_time": "2017-02-28T13:03:59.176Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_c5e7f54d4193\n",
       "\u001b[36mlrModel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionModel\u001b[39m = logreg_c5e7f54d4193"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.001)\n",
    "  .setLabelCol(\"IsArrDelayed\")\n",
    "  .setFeaturesCol(\"Features\") \n",
    "\n",
    "val lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:14:58.176670",
     "start_time": "2017-02-28T13:14:57.851Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m609\u001b[39m, \u001b[32m-18.64603660363778\u001b[39m),\n",
       "  (\u001b[32m308\u001b[39m, \u001b[32m13.4801612068689\u001b[39m),\n",
       "  (\u001b[32m306\u001b[39m, \u001b[32m-12.036568560945861\u001b[39m),\n",
       "  (\u001b[32m304\u001b[39m, \u001b[32m-11.126498828344797\u001b[39m),\n",
       "  (\u001b[32m287\u001b[39m, \u001b[32m-6.719433480968447\u001b[39m),\n",
       "  (\u001b[32m298\u001b[39m, \u001b[32m-6.321784069744478\u001b[39m),\n",
       "  (\u001b[32m268\u001b[39m, \u001b[32m-3.665292029649553\u001b[39m),\n",
       "  (\u001b[32m221\u001b[39m, \u001b[32m-2.8777270416209544\u001b[39m),\n",
       "  (\u001b[32m263\u001b[39m, \u001b[32m-2.869963711124296\u001b[39m),\n",
       "  (\u001b[32m302\u001b[39m, \u001b[32m2.656357508300985\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients.toArray.zipWithIndex.map(_.swap).sortBy(a => -math.abs(a._2)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model testing\n",
    "\n",
    "- We add a model prediction column to the testing dataset\n",
    "- We show the first 10 rows of the test dataset (with the new column)\n",
    "- We show the model ROC curve\n",
    "- We measure the model Area Under Curve (AUC) to be 0.706 on the testing dataset.  \n",
    "\n",
    "This is telling us that our model is not super accurate  (we generally assume that a model is raisonable at predicting when it has an AUC above 0.8).  But, since we are not trying to build the best possible model, but just show comparison of data science code/performance accross languages/libraries.\n",
    "If none the less you are willing to improve this result, you should try adding more feature column into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:21:16.590854",
     "start_time": "2017-02-28T13:21:15.990Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "|IsArrDelayed|            Features|       rawPrediction|         probability|prediction|\n",
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "|           1|(610,[0,1,2,3,4,5...|[1.86870909738161...|[0.86630883860640...|       0.0|\n",
      "|           1|(610,[0,1,2,3,4,5...|[1.47930175230465...|[0.81446709135184...|       0.0|\n",
      "|           0|(610,[0,1,2,3,4,5...|[1.78182058195163...|[0.85592152534880...|       0.0|\n",
      "|           1|(610,[0,1,2,3,4,5...|[1.87177312743705...|[0.86666330980389...|       0.0|\n",
      "|           1|(610,[0,1,2,3,4,5...|[0.98950749209980...|[0.72899063184229...|       0.0|\n",
      "|           0|(610,[0,1,2,3,4,5...|[0.98053362392551...|[0.72721408615902...|       0.0|\n",
      "|           1|(610,[0,1,2,3,4,5...|[-0.0419693560510...|[0.48950920083996...|       1.0|\n",
      "|           0|(610,[0,1,2,3,4,5...|[0.86983454048872...|[0.70471126814918...|       0.0|\n",
      "|           0|(610,[0,1,2,3,4,5...|[0.94747595823167...|[0.72060728964905...|       0.0|\n",
      "|           0|(610,[0,1,2,3,4,5...|[0.83304765134481...|[0.69699895377854...|       0.0|\n",
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtestWithPred\u001b[39m: \u001b[32mDataFrame\u001b[39m = [IsArrDelayed: int, Features: vector ... 3 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testWithPred = lrModel.transform(test.select(\"IsArrDelayed\",\"Features\"))\n",
    "testWithPred.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:22:48.918883",
     "start_time": "2017-02-28T13:22:47.528Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingSummary\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionSummary\u001b[39m = org.apache.spark.ml.classification.BinaryLogisticRegressionSummary@636ee3ee\n",
       "\u001b[36mbinarySummary\u001b[39m: \u001b[32mBinaryLogisticRegressionSummary\u001b[39m = org.apache.spark.ml.classification.BinaryLogisticRegressionSummary@636ee3ee\n",
       "\u001b[36mroc\u001b[39m: \u001b[32mDataFrame\u001b[39m = [FPR: double, TPR: double]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummary = lrModel.evaluate(test)\n",
    "val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "val roc = binarySummary.roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:23:00.429712",
     "start_time": "2017-02-28T13:22:59.986Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-709911267\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,0.0027274200226834935,0.007246115901848885,0.012061857526059013,0.017498694798999046,0.023061551478927755,0.02920949826273246,0.03558247970187409,0.041874448665094426,0.048463463373359494,0.055187498874826726,0.06241561200424865,0.06922065998163718,0.07680882855959818,0.0843519902064918,0.09177813383261023,0.09934829963814427,0.10717950564386916,0.11504671719444795,0.12301294399337498,0.1309161610888077,0.13925144472248727,0.14743370479053775,0.15541793436189172,0.16424829423731255,0.17286262084360993,0.18050479773885178,0.1892091382072839,0.19786847174464867,0.20663582191657515,0.2156462095162655,0.2246385943435289,0.23391002214341008,0.24300142221902174,0.25220984031540855,0.2614632653428628,0.2709687291842944,0.28055520550164725,0.28739625902388966,0.29710875474823123,0.3064611950240337,0.31619169352080223,0.3258321781554359,0.33551766972113706,0.34518515851441123,0.3546906223558428,0.3643041028318361,0.37436765261850324,0.3846922426053612,0.39545790051667956,0.4055484544619872,0.4158820458350586,0.4264676760221074,0.4285289934649936,0.43914162781068283,0.4493672025491926,0.4598628188741066,0.4701424019298972,0.480917061227429,0.49148468864205086,0.5017282661529876,0.5121878769330477,0.5230705528651413,0.5337191927556844,0.5447098853223397,0.5557095792752084,0.5667542801591445,0.5730822546672187,0.5839109222820315,0.594811600986552,0.6061893531603867,0.6173600734513115,0.6287288242389328,0.6398995445298576,0.6512862980899058,0.6624300142221902,0.6739697913478676,0.6857796100599493,0.6972923830269862,0.7088141573802366,0.7099663348155616,0.7217671521414298,0.7333879417430285,0.74540479233802,0.757484652636506,0.7698075503627558,0.7822204619511405,0.7947773957189407,0.8070102795830558,0.819378184240373,0.8318451041460385,0.8447530919761643,0.8546906223558428,0.8677066268205303,0.8805606063333753,0.8933425747565125,0.9068986623940086,0.9204367472590779,0.9337858030136641,0.947467910058149,0.961096008785353,0.9747871172160513,0.9883792103984014,1.0,1.0],\"y\":[0.0,0.01946560412516114,0.03659908590179304,0.05334583382163366,0.06928395640454706,0.0850580100785187,0.10007031524668933,0.1147896402203211,0.12961443806398687,0.14405250205086137,0.15831477792101253,0.1719207781553967,0.18420250791046525,0.19733973983358724,0.21053556779561702,0.223883745458807,0.23704441579749208,0.24986522911051212,0.26263916559240597,0.2752841907887027,0.2880112504394703,0.3001757881167233,0.31253955232626274,0.32516113910699634,0.3366811203562639,0.34848236259228876,0.35795148247978437,0.36963553263799365,0.38137817883511077,0.3929801945388492,0.40426579163248566,0.4155748271416852,0.42652056720965664,0.4377006914332591,0.44872846595570137,0.4596976444392359,0.4703386851048869,0.48087425290050395,0.48791749677721785,0.4982889956638931,0.5091292628618306,0.5194773233329427,0.5299425758818704,0.5403492323918903,0.5507793273174734,0.5614203679831243,0.5719207781553967,0.5818352279385913,0.5914098206961209,0.6004101722723544,0.6102894644322043,0.6198523379819524,0.6290870737138169,0.6312551271534045,0.6404547052619243,0.650158209305051,0.659510137114731,0.6691433259111684,0.6781319582796203,0.6873901324270479,0.6970701980546115,0.7064690026954178,0.7153170045704911,0.7244697058478847,0.7331770772295793,0.7418727294034924,0.7505097855384976,0.7554670104300949,0.7643853275518575,0.7732098910113676,0.7814133364584555,0.7898863236845189,0.7981014883393882,0.8065744755654518,0.814766201804758,0.8232743466541662,0.831266846361186,0.8389077698347591,0.8469354271651236,0.8549513652877065,0.8557131137935076,0.8633657564748622,0.8712527833118481,0.8786241650064456,0.8859135122465721,0.8928981600843783,0.8997538966365873,0.9064221258642916,0.9135122465721317,0.9204265791632485,0.9272120004687683,0.9334231805929919,0.9382866518223368,0.9443572014531818,0.9506386968240947,0.9570139458572601,0.9623813430212118,0.9677721786007266,0.973409117543654,0.978612445798664,0.9838860893003633,0.9890776983475917,0.9943982186804172,1.0,1.0],\"name\":\"ROC\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"ROC Curve\"};\n",
       "\n",
       "  Plotly.plot('plot-709911267', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfpr\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.0\u001b[39m,\n",
       "  \u001b[32m0.0027274200226834935\u001b[39m,\n",
       "  \u001b[32m0.007246115901848885\u001b[39m,\n",
       "  \u001b[32m0.012061857526059013\u001b[39m,\n",
       "  \u001b[32m0.017498694798999046\u001b[39m,\n",
       "  \u001b[32m0.023061551478927755\u001b[39m,\n",
       "  \u001b[32m0.02920949826273246\u001b[39m,\n",
       "  \u001b[32m0.03558247970187409\u001b[39m,\n",
       "  \u001b[32m0.041874448665094426\u001b[39m,\n",
       "  \u001b[32m0.048463463373359494\u001b[39m,\n",
       "  \u001b[32m0.055187498874826726\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mtpr\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.0\u001b[39m,\n",
       "  \u001b[32m0.01946560412516114\u001b[39m,\n",
       "  \u001b[32m0.03659908590179304\u001b[39m,\n",
       "  \u001b[32m0.05334583382163366\u001b[39m,\n",
       "  \u001b[32m0.06928395640454706\u001b[39m,\n",
       "  \u001b[32m0.0850580100785187\u001b[39m,\n",
       "  \u001b[32m0.10007031524668933\u001b[39m,\n",
       "  \u001b[32m0.1147896402203211\u001b[39m,\n",
       "  \u001b[32m0.12961443806398687\u001b[39m,\n",
       "  \u001b[32m0.14405250205086137\u001b[39m,\n",
       "  \u001b[32m0.15831477792101253\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres31_3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-709911267\"\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.JupyterScala.init()\n",
    "val fpr = roc.select(\"FPR\").rdd.map(_.getDouble(0)).collect.toSeq;\n",
    "val tpr = roc.select(\"TPR\").rdd.map(_.getDouble(0)).collect.toSeq;\n",
    "\n",
    "plotly.Scatter(fpr, tpr, name = \"ROC\").plot(title = \"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T14:23:04.438816",
     "start_time": "2017-02-28T13:23:04.254Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.6451332096083849\n"
     ]
    }
   ],
   "source": [
    "println(s\"areaUnderROC: ${binarySummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Key takeaways\n",
    "\n",
    "- We built a GLM model predicting airline delay probability\n",
    "- We train it on 800K rows in ~15sec on an intel i7-6700K (4.0 GHz)\n",
    "- We measure an AUC of 0.702, which is not super accurate but reasonable\n",
    "- We demonstrated a typical workflow in python language in a Jupyter notebook\n",
    "\n",
    "I might be biased, but I find the [pandas](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)/[scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) documentation particularly complete and easy to read.  In addition they are thousdands of recent examples/tutorials all over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (develop)",
   "language": "scala",
   "name": "scala-develop"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
